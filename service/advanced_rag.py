import os
from typing import List, Dict, Any
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableMap
from langchain.memory import ConversationBufferMemory
from langchain_core.messages import HumanMessage, AIMessage


class AdvancedRAGService:
    """
    ServiÃ§o RAG avanÃ§ado especializado em documentaÃ§Ã£o tÃ©cnica.

    Features:
    - MemÃ³ria conversacional para troubleshooting iterativo
    - CitaÃ§Ãµes com fontes (arquivo:linha)
    - Prompt otimizado para documentaÃ§Ã£o tÃ©cnica
    - Code-aware chunking
    - Retrieval com mais contexto (k=7)
    """

    def __init__(self):
        # Embeddings model
        self.embeddings = HuggingFaceEmbeddings(
            model_name="all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

        # LLM - usando Groq para respostas rÃ¡pidas
        self.llm = ChatGroq(
            api_key=os.getenv("GROQ_API_KEY"),
            model_name="llama-3.1-8b-instant",
            temperature=0.1,  # Baixa temperatura para respostas mais precisas
            streaming=True    # Habilitar streaming
        )

        # Text splitter otimizado para cÃ³digo
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1200,        # Chunks maiores para preservar contexto de cÃ³digo
            chunk_overlap=300,      # Overlap maior para nÃ£o quebrar exemplos
            separators=[
                "\n\n\n",           # SeparaÃ§Ãµes grandes (entre seÃ§Ãµes)
                "\n\n",             # ParÃ¡grafos
                "\n```\n",          # Fim de blocos de cÃ³digo
                "\n```",
                "\n##",             # Headers markdown
                "\n#",
                "\n",
                " ",
                ""
            ],
            length_function=len,
        )

        # Vector store e chain
        self.vectorstore = None
        self.chain = None
        self.retriever = None

        # MemÃ³ria conversacional
        self.memory = ConversationBufferMemory(
            return_messages=True,
            memory_key="chat_history",
            output_key="answer",
            max_token_limit=2000  # Limitar memÃ³ria para nÃ£o explodir o contexto
        )

        # Armazenar metadados dos documentos
        self.doc_metadata = {}

    def load_collection(self, collection_name: str) -> bool:
        """
        Carrega uma coleÃ§Ã£o de documentos com caching FAISS.

        Args:
            collection_name: Nome da coleÃ§Ã£o a carregar

        Returns:
            bool: True se carregou com sucesso, False caso contrÃ¡rio
        """
        collection_path = f"data/collections/{collection_name}"
        faiss_index_path = os.path.join(collection_path, "faiss_index")

        # Verificar se Ã­ndice FAISS existe
        if os.path.exists(faiss_index_path):
            print(f"âœ… Carregando Ã­ndice FAISS existente de: {faiss_index_path}")
            self.vectorstore = FAISS.load_local(
                faiss_index_path,
                self.embeddings,
                allow_dangerous_deserialization=True
            )
        else:
            print(f"âš ï¸ Criando novo Ã­ndice FAISS para '{collection_name}'...")

            # Carregar documentos
            loader = DirectoryLoader(
                collection_path,
                glob="**/*.md",
                loader_cls=TextLoader,
                loader_kwargs={"encoding": "utf-8"},
            )
            documents = loader.load()

            if not documents:
                print("âŒ Nenhum documento encontrado!")
                return False

            print(f"ğŸ“„ {len(documents)} documentos encontrados")

            # Split em chunks
            texts = self.text_splitter.split_documents(documents)
            print(f"âœ‚ï¸ {len(texts)} chunks criados")

            # Criar vectorstore
            self.vectorstore = FAISS.from_documents(texts, self.embeddings)

            # Salvar Ã­ndice
            print(f"ğŸ’¾ Salvando Ã­ndice FAISS em: {faiss_index_path}")
            self.vectorstore.save_local(faiss_index_path)

        # Configurar retriever com mais documentos
        self.retriever = self.vectorstore.as_retriever(
            search_type="mmr",  # Maximum Marginal Relevance - mais diversidade
            search_kwargs={
                "k": 7,           # Recuperar 7 documentos
                "fetch_k": 20,    # Buscar 20 e filtrar para 7 melhores
                "lambda_mult": 0.7  # Balance entre relevÃ¢ncia e diversidade
            }
        )

        # Criar chain com memÃ³ria
        self._create_chain()

        return True

    def _create_chain(self):
        """Cria a chain RAG com memÃ³ria conversacional."""

        # Prompt especializado para documentaÃ§Ã£o tÃ©cnica
        prompt = ChatPromptTemplate.from_messages([
            ("system", """VocÃª Ã© um assistente especializado em documentaÃ§Ã£o tÃ©cnica para desenvolvedores.

Seu objetivo Ã© ajudar programadores a entender e usar a documentaÃ§Ã£o de forma eficiente.

DIRETRIZES:
1. **PrecisÃ£o TÃ©cnica**: Seja exato e especÃ­fico. Desenvolvedores precisam de detalhes.
2. **Exemplos de CÃ³digo**: Sempre que possÃ­vel, inclua exemplos prÃ¡ticos de cÃ³digo.
3. **CitaÃ§Ãµes**: Ao referenciar informaÃ§Ãµes, mencione de qual documento veio.
4. **FormataÃ§Ã£o**: Use markdown para formatar cÃ³digo com syntax highlighting:
   - Use ```python, ```javascript, ```bash, etc.
   - Use `cÃ³digo inline` para nomes de funÃ§Ãµes, variÃ¡veis, etc.
5. **Contexto Conversacional**: Use o histÃ³rico da conversa para dar respostas contextuais.
6. **Honestidade**: Se algo nÃ£o estÃ¡ na documentaÃ§Ã£o, diga claramente "nÃ£o encontrei isso na documentaÃ§Ã£o fornecida".

RESPONDA BASEADO NO CONTEXTO DOS DOCUMENTOS ABAIXO:

{context}

Se a pergunta envolver conceitos da conversa anterior, use o histÃ³rico para dar uma resposta mais completa."""),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{question}")
        ])

        # FunÃ§Ã£o para formatar documentos recuperados com fontes
        def format_docs_with_sources(docs):
            formatted = []
            sources = []

            for i, doc in enumerate(docs, 1):
                # Extrair nome do arquivo da metadata
                source = doc.metadata.get('source', 'unknown')
                file_name = os.path.basename(source)

                # Formatar documento com fonte
                formatted.append(f"[Documento {i} - {file_name}]\n{doc.page_content}\n")
                sources.append(f"- {file_name}")

            # Adicionar lista de fontes no final
            formatted.append(f"\nğŸ“š Fontes consultadas:\n" + "\n".join(set(sources)))

            return "\n".join(formatted)

        # FunÃ§Ã£o para converter histÃ³rico
        def get_chat_history():
            history = self.memory.load_memory_variables({})
            return history.get("chat_history", [])

        # Criar chain com memÃ³ria
        self.chain = (
            RunnableMap({
                "context": self.retriever | format_docs_with_sources,
                "question": RunnablePassthrough(),
                "chat_history": lambda x: get_chat_history(),
            })
            | prompt
            | self.llm
            | StrOutputParser()
        )

    def ask_question(self, question: str) -> str:
        """
        Faz uma pergunta ao sistema RAG.

        Args:
            question: Pergunta do usuÃ¡rio

        Returns:
            str: Resposta gerada
        """
        if not self.chain:
            return "âŒ ColeÃ§Ã£o nÃ£o carregada. Por favor, carregue uma coleÃ§Ã£o primeiro."

        try:
            # Invocar chain
            response = self.chain.invoke(question)

            # Salvar na memÃ³ria
            self.memory.save_context(
                {"input": question},
                {"answer": response}
            )

            return response

        except Exception as e:
            print(f"âŒ Erro ao processar pergunta: {e}")
            return f"âŒ Erro ao processar sua pergunta: {str(e)}"

    def ask_question_stream(self, question: str):
        """
        Faz uma pergunta com streaming de resposta.

        Args:
            question: Pergunta do usuÃ¡rio

        Yields:
            str: Chunks da resposta sendo gerada
        """
        if not self.chain:
            yield "âŒ ColeÃ§Ã£o nÃ£o carregada. Por favor, carregue uma coleÃ§Ã£o primeiro."
            return

        try:
            full_response = ""

            # Stream a resposta
            for chunk in self.chain.stream(question):
                full_response += chunk
                yield chunk

            # Salvar na memÃ³ria apÃ³s completar
            self.memory.save_context(
                {"input": question},
                {"answer": full_response}
            )

        except Exception as e:
            print(f"âŒ Erro ao processar pergunta: {e}")
            yield f"\n\nâŒ Erro ao processar sua pergunta: {str(e)}"

    def get_relevant_docs(self, question: str, k: int = 5) -> List[Dict[str, Any]]:
        """
        Retorna documentos relevantes para uma pergunta.

        Args:
            question: Pergunta do usuÃ¡rio
            k: NÃºmero de documentos a retornar

        Returns:
            Lista de documentos com metadata
        """
        if not self.retriever:
            return []

        try:
            docs = self.retriever.get_relevant_documents(question)[:k]

            results = []
            for doc in docs:
                results.append({
                    "content": doc.page_content,
                    "source": os.path.basename(doc.metadata.get('source', 'unknown')),
                    "metadata": doc.metadata
                })

            return results

        except Exception as e:
            print(f"âŒ Erro ao buscar documentos: {e}")
            return []

    def clear_memory(self):
        """Limpa a memÃ³ria conversacional."""
        self.memory.clear()
        print("ğŸ§¹ MemÃ³ria conversacional limpa!")

    def get_memory_summary(self) -> str:
        """Retorna um resumo da memÃ³ria conversacional."""
        history = self.memory.load_memory_variables({})
        messages = history.get("chat_history", [])

        if not messages:
            return "ğŸ“­ Nenhuma conversa no histÃ³rico."

        summary = f"ğŸ’¬ HistÃ³rico: {len(messages)//2} perguntas e respostas\n\n"

        # Mostrar Ãºltimas 3 interaÃ§Ãµes
        recent = messages[-6:] if len(messages) > 6 else messages

        for msg in recent:
            if isinstance(msg, HumanMessage):
                summary += f"ğŸ‘¤ **VocÃª**: {msg.content[:100]}...\n"
            elif isinstance(msg, AIMessage):
                summary += f"ğŸ¤– **Assistente**: {msg.content[:100]}...\n\n"

        return summary
