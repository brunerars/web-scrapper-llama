# ü§ñ Web Scrapper Llama: RAG com Streamlit e Groq

Este projeto √© uma aplica√ß√£o web interativa constru√≠da com **Streamlit** que combina funcionalidades de **Web Scraping** e **Gera√ß√£o Aumentada por Recupera√ß√£o (RAG)**. O objetivo √© permitir a cria√ß√£o de "cole√ß√µes" de documentos a partir de URLs e, em seguida, interagir com esse conte√∫do via chat, utilizando um Large Language Model (LLM) atrav√©s da API Groq.

O projeto √© modular e focado em facilitar a expans√£o e manuten√ß√£o por desenvolvedores.

## ‚öôÔ∏è Tecnologias Chave

A base do projeto √© constru√≠da sobre um *stack* moderno de Python, focado em desenvolvimento r√°pido e capacidades de IA.

| Categoria | Tecnologia | Prop√≥sito |
| :--- | :--- | :--- |
| **Frontend/App** | `Streamlit` | Cria√ß√£o da interface web interativa e do fluxo da aplica√ß√£o. |
| **Core LLM** | `Groq` (via `langchain-groq`) | Provedor de LLM de alta velocidade para as intera√ß√µes de Chat (RAG). |
| **Framework de IA** | `LangChain` | Orquestra√ß√£o do fluxo RAG, manipula√ß√£o de documentos e gerenciamento de *chains*. |
| **Web Scraping** | `Firecrawl` | Servi√ßo de scraping para extrair conte√∫do limpo de URLs. |
| **Armazenamento Vetorial** | `FAISS` | Banco de dados vetorial local para indexa√ß√£o e recupera√ß√£o r√°pida dos documentos. |
| **Gerenciamento de Ambiente** | `python-dotenv` | Carregamento de vari√°veis de ambiente (como chaves de API) a partir do arquivo `.env`. |

## üìÇ Estrutura do Projeto

A organiza√ß√£o do c√≥digo segue uma separa√ß√£o clara de responsabilidades:

```
web-scrapper-llama/
‚îú‚îÄ‚îÄ app.py                  # Ponto de entrada principal da aplica√ß√£o Streamlit.
‚îú‚îÄ‚îÄ requirements.txt        # Lista de todas as depend√™ncias do projeto.
‚îú‚îÄ‚îÄ .env.example            # Exemplo de arquivo de configura√ß√£o de vari√°veis de ambiente.
‚îú‚îÄ‚îÄ data/                   # Diret√≥rio para armazenamento das cole√ß√µes de documentos.
‚îÇ   ‚îî‚îÄ‚îÄ collections/        # Cole√ß√µes de documentos indexados.
‚îÇ       ‚îî‚îÄ‚îÄ Agno/           # Exemplo de uma cole√ß√£o.
‚îÇ           ‚îú‚îÄ‚îÄ faiss_index/  # Arquivos de √≠ndice vetorial FAISS.
‚îÇ           ‚îî‚îÄ‚îÄ page_*.md     # Documentos de origem (p√°ginas web raspadas).
‚îú‚îÄ‚îÄ presentation/           # M√≥dulos de l√≥gica de apresenta√ß√£o (Streamlit UI).
‚îÇ   ‚îú‚îÄ‚îÄ chat.py             # L√≥gica de interface e intera√ß√£o do modo Chat.
‚îÇ   ‚îî‚îÄ‚îÄ scrapping.py        # L√≥gica de interface e intera√ß√£o do modo Scrapping.
‚îî‚îÄ‚îÄ service/                # M√≥dulos de l√≥gica de neg√≥cio e servi√ßos.
    ‚îú‚îÄ‚îÄ rag.py              # Implementa√ß√£o do fluxo RAG (indexa√ß√£o e consulta).
    ‚îî‚îÄ‚îÄ scrapping.py        # L√≥gica de servi√ßo para a chamada do Firecrawl.
```

## üöÄ Primeiros Passos

Siga estas etapas para configurar e executar o projeto localmente.

### 1. Pr√©-requisitos

*   **Python 3.10+**
*   **Chave de API Groq:** Necess√°ria para o funcionamento do LLM.
*   **Chave de API Firecrawl:** Necess√°ria para a funcionalidade de Web Scraping.

### 2. Configura√ß√£o do Ambiente

1.  **Clone o reposit√≥rio:**
    ```bash
    git clone https://github.com/brunerars/web-scrapper-llama.git
    cd web-scrapper-llama
    ```

2.  **Crie e ative um ambiente virtual (recomendado):**
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # No Linux/macOS
    # venv\Scripts\activate   # No Windows
    ```

3.  **Instale as depend√™ncias:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Configure as Vari√°veis de Ambiente:**
    Crie um arquivo chamado `.env` na raiz do projeto, baseado no `.env.example`, e preencha com suas chaves de API.

    **.env**
    ```
    GROQ_API_KEY="SUA_CHAVE_GROQ_AQUI"
    FIRECRAWL_API_KEY="SUA_CHAVE_FIRECRAWL_AQUI"
    ```

### 3. Execu√ß√£o da Aplica√ß√£o

Inicie a aplica√ß√£o Streamlit a partir do diret√≥rio raiz do projeto:

```bash
streamlit run app.py
```

A aplica√ß√£o ser√° aberta automaticamente no seu navegador padr√£o (geralmente em `http://localhost:8501`).

## üõ†Ô∏è Fluxo de Trabalho

O projeto opera em dois modos principais, selecion√°veis na barra lateral:

### Modo Scrapping

1.  O usu√°rio insere uma URL e um nome para a nova **Cole√ß√£o**.
2.  O servi√ßo `service/scrapping.py` utiliza a API **Firecrawl** para raspar o conte√∫do da URL.
3.  O conte√∫do raspado √© processado e dividido em documentos.
4.  O servi√ßo `service/rag.py` cria um √≠ndice vetorial **FAISS** a partir desses documentos e o armazena em `data/collections/{NomeDaColecao}/`.

### Modo Chat (RAG)

1.  O usu√°rio seleciona uma **Cole√ß√£o** existente na barra lateral.
2.  O √≠ndice FAISS da cole√ß√£o √© carregado.
3.  O usu√°rio faz uma pergunta.
4.  O m√≥dulo `presentation/chat.py` utiliza o fluxo **LangChain RAG** para:
    a.  Buscar os documentos mais relevantes no √≠ndice FAISS.
    b.  Passar a pergunta do usu√°rio e os documentos recuperados como contexto para o LLM (Groq).
    c.  O LLM gera uma resposta baseada no contexto fornecido.

## ü§ù Contribui√ß√£o

Sinta-se √† vontade para clonar, bifurcar e contribuir para este projeto. As principais √°reas de desenvolvimento incluem:

*   **Melhoria da UI/UX:** Refinamento da interface Streamlit.
*   **Novos Servi√ßos de Scraping:** Integra√ß√£o com outras ferramentas de extra√ß√£o de dados.
*   **Otimiza√ß√£o do RAG:** Experimenta√ß√£o com diferentes *chunking strategies* e modelos de *embedding*.
*   **Persist√™ncia de Cole√ß√µes:** Implementa√ß√£o de um banco de dados mais robusto para metadados das cole√ß√µes.

Para contribuir, siga o fluxo padr√£o de **Git**: crie uma *branch* para sua *feature* ou corre√ß√£o e abra um *Pull Request* para a *branch* principal.
